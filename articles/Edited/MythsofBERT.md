

# Myths of BERT

---

| Question   | Answer                                                            |
| ---------- | ----------------------------------------------------------------- |
| Writer     |Ashwani Varshney - MSc II year                               |
| Editor     | Ashwani                                                      |
| Status     | Edited |
| Plagiarism | 0%. [Report](https://github.com/RishPoria/Srijan-2021/blob/main/articles/plagReports/Myths%20of%20BERT.pdf)|

On October 25, Google released its much-anticipated algorithm update, namely BERT. BERT is anticipated to be a disruptive force, hailed as a significant milestone in search engine history. The backbone of this new update is Google’s new technology, BERT, which it introduced in the year 2018. BERT short for Bidirectional Encoder Representation from Transformers. Here, transformers refer to models that interpret the meaning of conjoining words concerning each other. It means that now the search engine giant can understand the proper context and sematic of a search query by analyzing the correlation of words and offer better search results. It is a massive bonus for users and goes a long way in establishing user’s trust and more excellent brand value.

## What is BERT?

BERT is Google’s latest deep learning algorithm related to Natural Language Processing (NLP). Using the BERT algorithm, Google can better understand search queries as BERT can decipher the exact semantic and context of the words used in a phrase or sentence. According to the search engine expert Bill Slawski, BERT is an NLP pre-training approach used on a large body of text. It manages tasks such as Named Entity Recognition (NER), part-of-speech (POS) tagging, and answering the questions, among other natural language tasks. BERT aids Google in understanding natural language text from the Web. Google has open-sourced this technology, and many have created variations of BERT.

## How will Search Engine Optimization be affected by BERT? 

Of course, BERT will impact rankings and featured snippets like other Google algorithm updates, but the good news is that Google will not use BERT for all queries. Currently, BERT will be used only in 10 percent of searches in U.S English. As per Google, BERT is exceptionally complicated in that it pushes the boundaries of Google’s hardware; therefore, it has been constrained to fewer searches.

Google is deploying the BERT model in 24 countries where featured snippets are used to improve the quality of the snippets. So, there will not be significant upheavals in the search ranking; however, after the post-full-fledged implementation of BERT, website owners have to develop highly optimized content to better rank on search engine pages.

## The myths surrounding BERT update

Despite clear indications from Google, many failed to understand the BERT update in its entirety, which resulted in the unwanted content that disseminated information that was nothing but myths. Now, it is time to bust those myths and get an accurate picture. Let us have a look at some of the myths that made headlines.

### 1.	With the BERT update, one will have to optimize the website for long-tail keywords.

It is one of the most pervasive myths about BERT. BERT understands search queries, including those written in natural language and those with prepositions and “stop words” that add to the semantic of the query. Some people have misconstrued this fact and assumed that it is now essential to optimize internet sites for long-tail queries. Nevertheless, BERT was designed to understand users’ intent and connect to the information on the websites. There is no need to overhaul the content –continue writing great content, as always.

### 2.	BERT increases the significance of stop-words.

Stop words are the words that are typically preprocessed by NLP tools. Although there is no universal list of stop-words, they are the most frequently occurring words in a language. As mentioned, BERT interprets those queries better, which are written in natural language and that use stop words. Some articles have misconstrued that stop words are more worthy now and should be included in the content. Updating the content to add more stop words will not help the SEO in any way.

### 3.	The BERT update is not that significant

Well, what can be said about such obliviousness? BERT is already affecting ten percent of English language search queries. Is this not a significant number? The perception that the BERT is a minor update is likely a reflection that it is not shaking up many keywords. Instead, it is assisting with keywords that may have been misinterpreted previously. Depending on the interpretation, one might consider BERT to be relatively large (because it affects about 10 percent of queries) or relatively small (because it has not reshuffled many valuable two- to three-word phrases).

### 4.	BERT is the most significant update of all time

It is based on a press release from Google that stated that it is one of the quantum leaps forward in the history of search engines. Even in this statement, BERT remains one of the most extensive updates and not an all-time big update.

With Google’s BERT update, the search engine can understand the nuances of language and is undoubtedly a massive step in delivering the search results. In this procedure, Google will provide its users with a first-class search experience. It is still under anticipation how BERT will alter Google’s search page ranking as the update is applied on only one out of ten searches; now, the effect seems minimal. However, it must be noted that once the full implementation of the BERT update, content creators have to tighten their backs to come up with unique and highly optimized content to rank better on result pages.

Although Google aims to improve the search query results with BERT, understanding the entire language is still challenging. Despite the efforts undertaken, BERT might not get everything correct just yet. However, the industry giant is poised for similar efforts to enhance the search results and provide a great user experience. This deep neural network-based technique for NLP tasks helps the algorithm understand the context and subtleties of users’ queries. Eventually, the goal is to deliver more relevant results to the end-users while encouraging them to use natural language rather than keyword-heavy queries.
